{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8750cb60-d582-4493-9954-38f517c4002c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score,classification_report,recall_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier,VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "437527c7-61ae-4967-85fa-510a151786ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Basline Model\n",
    "df=pd.read_csv(\"novagen_dataset.csv\")\n",
    "df.isnull().sum()\n",
    "\n",
    "#split features and target\n",
    "X=df.drop(\"Target\",axis=1)\n",
    "y=df[\"Target\"]\n",
    "\n",
    "# What is stratify in train_test_split?\n",
    "# stratify=y means:\n",
    "# Split the data in such a way that the class \n",
    "# proportion in train and test remains the same as the original dataset.\n",
    "#  Simple Meaning\n",
    "# If your dataset has:\n",
    "# 70% Class 0\n",
    "# 30% Class 1\n",
    "# Then after splitting:\n",
    "# Train set → 70% Class 0, 30% Class 1\n",
    "# Test set → 70% Class 0, 30% Class 1\n",
    "# So no class imbalance is introduced by splitting.\n",
    "\n",
    "#train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d2d0076-642d-4530-b4c5-905cc03d5445",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Algorithm Wise Rule:\n",
    "\n",
    "# | Algorithm           | Scale X? | Scale y? |\n",
    "# | ------------------- | -------- | -------- |\n",
    "# | Logistic Regression | ✅ Yes    | ❌ No  |\n",
    "# | KNN                 | ✅ Yes    | ❌ No  |\n",
    "# | SVM                 | ✅ Yes    | ❌ No  |\n",
    "# | Decision Tree       | ❌ No     | ❌ No  |\n",
    "# | Random Forest       | ❌ No     | ❌ No  |\n",
    "# | Linear Regression   | Optional |  ❌ No   |\n",
    "# | SVR                 | ✅ Yes    | ✅ Yes |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54404bd8-6c22-455b-a436-92110ed8b6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaling(imp in logistic and KNN Because they are distance based)\n",
    "scaler = StandardScaler()\n",
    "# Because X contains features (inputs) and y contains labels (outputs).\n",
    "# Scaling is needed for inputs, not class labels.\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e274e527-7b07-498b-9c31-c240ff7238b8",
   "metadata": {},
   "source": [
    "# Logistic Regression (With regulaization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3396f520-7b09-4101-862c-55d354de02bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logisitic regression Accuracy: 0.8141361256544503\n",
      "Logisitic regression recall: 0.8283132530120482\n",
      "Logisitic regression classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.80      0.80       914\n",
      "           1       0.82      0.83      0.82       996\n",
      "\n",
      "    accuracy                           0.81      1910\n",
      "   macro avg       0.81      0.81      0.81      1910\n",
      "weighted avg       0.81      0.81      0.81      1910\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_reg=LogisticRegression(\n",
    "    penalty=\"l2\",\n",
    "    solver=\"liblinear\",\n",
    "    max_iter=1000\n",
    ")\n",
    "#Training\n",
    "log_reg.fit(X_train_scaled,y_train)\n",
    "#Prediciting\n",
    "y_pred_log_r=log_reg.predict(X_test_scaled)\n",
    "\n",
    "# In Model Evaluation, Recall is more important than accuracy \n",
    "# because missing a high-risk patient is dangerous\n",
    "\n",
    "print(\"Logisitic regression Accuracy:\",accuracy_score(y_test,y_pred_log_r))\n",
    "print(\"Logisitic regression recall:\",recall_score(y_test,y_pred_log_r))\n",
    "print(\"Logisitic regression classification report:\\n\",classification_report(y_test,y_pred_log_r))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575f9ad8-b564-4006-b35c-ff399a2a5d87",
   "metadata": {},
   "source": [
    "# Model 2 KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69095870-9002-4aaf-9291-812f64956754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Accuracy: 0.8832460732984293\n",
      "KNN Recall: 0.8835341365461847\n",
      "KNN classification Report               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.88      0.88       914\n",
      "           1       0.89      0.88      0.89       996\n",
      "\n",
      "    accuracy                           0.88      1910\n",
      "   macro avg       0.88      0.88      0.88      1910\n",
      "weighted avg       0.88      0.88      0.88      1910\n",
      "\n"
     ]
    }
   ],
   "source": [
    "knn=KNeighborsClassifier(\n",
    "    n_neighbors=5,\n",
    "    metric=\"euclidean\"#Distance is calculated using Euclidean distance (straight-line distance)\n",
    ")\n",
    "knn.fit(X_train_scaled,y_train)\n",
    "y_pred_knn=knn.predict(X_test_scaled)\n",
    "\n",
    "print(\"KNN Accuracy:\", accuracy_score(y_test, y_pred_knn))\n",
    "print(\"KNN Recall:\", recall_score(y_test, y_pred_knn))\n",
    "print(\"KNN classification Report\",classification_report(y_test, y_pred_knn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3989b5a7-3c45-4c1f-acaf-49880c8c8353",
   "metadata": {},
   "source": [
    "# Random Forest(Ensemble learning Bagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7c89c94-2e53-49a9-a48b-51408dd643fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.9382198952879581\n",
      "Random Forest Recall: 0.9588353413654619\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.92      0.93       914\n",
      "           1       0.93      0.96      0.94       996\n",
      "\n",
      "    accuracy                           0.94      1910\n",
      "   macro avg       0.94      0.94      0.94      1910\n",
      "weighted avg       0.94      0.94      0.94      1910\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf=RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=None,\n",
    "    random_state=42\n",
    ")\n",
    "#training\n",
    "rf.fit(X_train,y_train)\n",
    "#predictions\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "print(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
    "print(\"Random Forest Recall:\", recall_score(y_test, y_pred_rf))\n",
    "print(classification_report(y_test, y_pred_rf))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6f06d9-73e2-404c-9743-f5d8df745998",
   "metadata": {},
   "source": [
    "# Gradient Boosting (Ensemble Learning Homogenous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffeadf59-f7ce-4d01-b2a6-7e475cfdefa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting Accuracy: 0.9303664921465968\n",
      "Gradient Boosting Recall: 0.9497991967871486\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.91      0.93       914\n",
      "           1       0.92      0.95      0.93       996\n",
      "\n",
      "    accuracy                           0.93      1910\n",
      "   macro avg       0.93      0.93      0.93      1910\n",
      "weighted avg       0.93      0.93      0.93      1910\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gb = GradientBoostingClassifier(\n",
    "    n_estimators=150,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "gb.fit(X_train, y_train)\n",
    "\n",
    "y_pred_gb = gb.predict(X_test)\n",
    "\n",
    "print(\"Gradient Boosting Accuracy:\", accuracy_score(y_test, y_pred_gb))\n",
    "print(\"Gradient Boosting Recall:\", recall_score(y_test, y_pred_gb))\n",
    "print(classification_report(y_test, y_pred_gb))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a29ffa-46cd-4cff-ae1f-4a831e943218",
   "metadata": {},
   "source": [
    "# Voting classifier(Ensemble learning Hetrogenus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a9157ad-325b-4c34-acfc-c8addfb03d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting Classifier Accuracy: 0.9157068062827225\n",
      "Voting Classifier Recall: 0.929718875502008\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.90      0.91       914\n",
      "           1       0.91      0.93      0.92       996\n",
      "\n",
      "    accuracy                           0.92      1910\n",
      "   macro avg       0.92      0.92      0.92      1910\n",
      "weighted avg       0.92      0.92      0.92      1910\n",
      "\n"
     ]
    }
   ],
   "source": [
    "voting_clf = VotingClassifier(\n",
    "    estimators=[\n",
    "        (\"lr\", LogisticRegression(max_iter=1000, solver=\"liblinear\")),\n",
    "        (\"knn\", KNeighborsClassifier(n_neighbors=5)),\n",
    "        (\"rf\", RandomForestClassifier(n_estimators=200, random_state=42))\n",
    "    ],\n",
    "    voting=\"soft\"\n",
    ")\n",
    "\n",
    "voting_clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred_vote = voting_clf.predict(X_test_scaled)\n",
    "\n",
    "print(\"Voting Classifier Accuracy:\", accuracy_score(y_test, y_pred_vote))\n",
    "print(\"Voting Classifier Recall:\", recall_score(y_test, y_pred_vote))\n",
    "print(classification_report(y_test, y_pred_vote))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ad789c-3b5e-4689-9e54-277d583bfb1f",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "| Model                | Recall |\n",
    "|----------------------|:------:|\n",
    "| Logistic Regression  | 82.8%  |\n",
    "| KNN                  | 88.3%  |\n",
    "| Random Forest        | 95.8%  |\n",
    "| Gradient Boosting    | 94.9%  |\n",
    "| Voting Classifier    | 93.07% |\n",
    "\n",
    "### Best Classifier that we should use for NovaGen(based on Recall) - Random Forest with accuracy of 93.7%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2ea6fa-deef-465f-9e2f-1c0f37ba9a2b",
   "metadata": {},
   "source": [
    "# Why Recall is More Important Than Accuracy?\n",
    "# This is the most important part of project.\n",
    "#  Medical Context Reality\n",
    "# Positive class (1) = High-risk patient\n",
    "# False Negative = Model says “low risk” but patient is actually high-risk \n",
    "#  This is dangerous.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
